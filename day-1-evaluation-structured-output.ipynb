{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-01T11:20:31.751535Z",
     "iopub.status.busy": "2025-04-01T11:20:31.751166Z",
     "iopub.status.idle": "2025-04-01T11:20:38.891195Z",
     "shell.execute_reply": "2025-04-01T11:20:38.890056Z",
     "shell.execute_reply.started": "2025-04-01T11:20:31.751489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -Uq \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T11:22:42.805467Z",
     "iopub.status.busy": "2025-04-01T11:22:42.805109Z",
     "iopub.status.idle": "2025-04-01T11:22:44.455534Z",
     "shell.execute_reply": "2025-04-01T11:22:44.454706Z",
     "shell.execute_reply.started": "2025-04-01T11:22:42.805443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "genai.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T11:23:02.125828Z",
     "iopub.status.busy": "2025-04-01T11:23:02.125429Z",
     "iopub.status.idle": "2025-04-01T11:23:02.393258Z",
     "shell.execute_reply": "2025-04-01T11:23:02.392422Z",
     "shell.execute_reply.started": "2025-04-01T11:23:02.125802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "client = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T11:23:45.086588Z",
     "iopub.status.busy": "2025-04-01T11:23:45.086062Z",
     "iopub.status.idle": "2025-04-01T11:23:45.091853Z",
     "shell.execute_reply": "2025-04-01T11:23:45.090615Z",
     "shell.execute_reply.started": "2025-04-01T11:23:45.086557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "if not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n",
    "  genai.models.Models.generate_content = retry.Retry(\n",
    "      predicate=is_retriable)(genai.models.Models.generate_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T11:24:49.684237Z",
     "iopub.status.busy": "2025-04-01T11:24:49.683912Z",
     "iopub.status.idle": "2025-04-01T11:24:51.748895Z",
     "shell.execute_reply": "2025-04-01T11:24:51.747972Z",
     "shell.execute_reply.started": "2025-04-01T11:24:49.684212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-01 11:24:50 URL:https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf [7228817/7228817] -> \"gemini.pdf\" [1]\n"
     ]
    }
   ],
   "source": [
    "!wget -nv -O gemini.pdf https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf\n",
    "\n",
    "document_file = client.files.upload(file='gemini.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T11:26:57.906005Z",
     "iopub.status.busy": "2025-04-01T11:26:57.905688Z",
     "iopub.status.idle": "2025-04-01T11:27:08.628093Z",
     "shell.execute_reply": "2025-04-01T11:27:08.627157Z",
     "shell.execute_reply.started": "2025-04-01T11:26:57.905981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's analyze the training process outlined in this document for Gemini 1.5.\n",
       "\n",
       "**Training Infrastructure:**\n",
       "\n",
       "*   **TPUs:**  The Gemini 1.5 Pro model is trained on Google's Tensor Processing Units (TPUs), specifically TPUv4, which suggests large-scale, purpose-built hardware for accelerating the training. 4096-chip pods used during training, which highlights a compute-heavy and distributed training strategy.\n",
       "\n",
       "**Datasets and Methods:**\n",
       "\n",
       "*   **Multimodal Training:** The model was pre-trained on diverse, sourced from various domains including web documents, code, and data across modalities of data (images, audio, video content).\n",
       "\n",
       "*   **Multilingual Training:** Pre-training involved training on multimodal and multilingual datasets.\n",
       "\n",
       "*   **Instruction tuning /Fine Tuning Phase:** After pre-training, a finetuning was undertaken which used supervised fine tuning using instructions, on a multimodal set and was supplemented further based upon Human Preference Data\n",
       "\n",
       "**Key Techniques:**\n",
       "\n",
       "*   **Mixture of Experts (MoE) Architecture:**  Gemini 1.5 Pro uses MoE. The architecture directs inputs to only a subset of the network's parameters, for which parameters have a fixed compute allocation.\n",
       "\n",
       "I hope this breakdown is helpful!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = 'Tell me about the training process used here.'\n",
    "\n",
    "def summarise_doc(request: str) -> str:\n",
    "  \"\"\"Execute the request on the uploaded document.\"\"\"\n",
    "  # Set the temperature low to stabilise the output.\n",
    "  config = types.GenerateContentConfig(temperature=2.0)\n",
    "  response = client.models.generate_content(\n",
    "      model='gemini-2.0-flash',\n",
    "      config=config,\n",
    "      contents=[request, document_file],\n",
    "  )\n",
    "\n",
    "  return response.text\n",
    "\n",
    "summary = summarise_doc(request)\n",
    "Markdown(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T11:53:11.478661Z",
     "iopub.status.busy": "2025-04-01T11:53:11.478200Z",
     "iopub.status.idle": "2025-04-01T11:53:11.490407Z",
     "shell.execute_reply": "2025-04-01T11:53:11.489566Z",
     "shell.execute_reply.started": "2025-04-01T11:53:11.478631Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## What is 2+2?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_help: StructEval(value=4)\n",
      "with_help: StructEval(value=4)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Explain gravity in simple terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_help: StructEval(value=4)\n",
      "with_help: StructEval(value=4)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Number of times to repeat each task in order to reduce error and calculate an average.\n",
    "# Increasing it will take longer but give better results, try 2 or 3 to start.\n",
    "NUM_ITERATIONS = 1\n",
    "\n",
    "scores = collections.defaultdict(int)\n",
    "responses = collections.defaultdict(list)\n",
    "\n",
    "for question in questions:\n",
    "  display(Markdown(f'## {question}'))\n",
    "  for guidance, guide_prompt in guidance_options.items():\n",
    "\n",
    "    for n in range(NUM_ITERATIONS):\n",
    "      # Generate a response.\n",
    "      answer = answer_question(question, guide_prompt)\n",
    "      # Evaluate the response (note that the guidance prompt is not passed).\n",
    "      written_eval, struct_eval = eval_answer(question, answer, n)\n",
    "      print(f'{guidance}: {struct_eval}')\n",
    "\n",
    "      # Save the numeric score.\n",
    "      scores[guidance] += int(struct_eval.value)\n",
    "\n",
    "      # Save the responses, in case you wish to inspect them.\n",
    "      responses[(guidance, question)].append((answer, written_eval))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:06:47.568151Z",
     "iopub.status.busy": "2025-04-01T12:06:47.567837Z",
     "iopub.status.idle": "2025-04-01T12:06:47.573688Z",
     "shell.execute_reply": "2025-04-01T12:06:47.572519Z",
     "shell.execute_reply.started": "2025-04-01T12:06:47.568127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class AnswerRating(Enum):\n",
    "    POOR = \"1\"\n",
    "    AVERAGE = \"2\"\n",
    "    GOOD = \"3\"\n",
    "    EXCELLENT = \"4\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_score(cls, score):\n",
    "        rounded = str(round(score))  # Convert rounded score to string\n",
    "        return cls(rounded) if rounded in cls._value2member_map_ else cls.POOR  # Default to POOR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T12:06:50.934367Z",
     "iopub.status.busy": "2025-04-01T12:06:50.934024Z",
     "iopub.status.idle": "2025-04-01T12:06:50.940376Z",
     "shell.execute_reply": "2025-04-01T12:06:50.939212Z",
     "shell.execute_reply.started": "2025-04-01T12:06:50.934339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_help: 4.00 - EXCELLENT\n",
      "with_help: 4.00 - EXCELLENT\n"
     ]
    }
   ],
   "source": [
    "for guidance, score in scores.items():\n",
    "  avg_score = score / (NUM_ITERATIONS * len(questions))\n",
    "  nearest = AnswerRating(str(round(avg_score)))\n",
    "  print(f'{guidance}: {avg_score:.2f} - {nearest.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-02T11:56:56.176214Z",
     "iopub.status.busy": "2025-04-02T11:56:56.175821Z",
     "iopub.status.idle": "2025-04-02T11:56:56.185180Z",
     "shell.execute_reply": "2025-04-02T11:56:56.184216Z",
     "shell.execute_reply.started": "2025-04-02T11:56:56.176146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "NUM_ITERATIONS = 1  # or any default value\n",
    "\n",
    "@functools.total_ordering\n",
    "class QAGuidancePrompt:\n",
    "  \"\"\"A question-answering guidance prompt or system instruction.\"\"\"\n",
    "\n",
    "  def __init__(self, prompt, questions, n_comparisons=NUM_ITERATIONS):\n",
    "    \"\"\"Create the prompt. Provide questions to evaluate against, and number of evals to perform.\"\"\"\n",
    "    self.prompt = prompt\n",
    "    self.questions = questions\n",
    "    self.n = n_comparisons\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.prompt\n",
    "\n",
    "  def _compare_all(self, other):\n",
    "    \"\"\"Compare two prompts on all questions over n trials.\"\"\"\n",
    "    results = [self._compare_n(other, q) for q in questions]\n",
    "    mean = sum(results) / len(results)\n",
    "    return round(mean)\n",
    "\n",
    "  def _compare_n(self, other, question):\n",
    "    \"\"\"Compare two prompts on a question over n trials.\"\"\"\n",
    "    results = [self._compare(other, question, n) for n in range(self.n)]\n",
    "    mean = sum(results) / len(results)\n",
    "    return mean\n",
    "\n",
    "  def _compare(self, other, question, n=1):\n",
    "    \"\"\"Compare two prompts on a single question.\"\"\"\n",
    "    answer_a = answer_question(question, self.prompt)\n",
    "    answer_b = answer_question(question, other.prompt)\n",
    "\n",
    "    _, result = eval_pairwise(\n",
    "        prompt=question,\n",
    "        response_a=answer_a,\n",
    "        response_b=answer_b,\n",
    "        n=n,  # Cache buster\n",
    "    )\n",
    "    # print(f'q[{question}], a[{self.prompt[:20]}...], b[{other.prompt[:20]}...]: {result}')\n",
    "\n",
    "    # Convert the enum to the standard Python numeric comparison values.\n",
    "    if result is AnswerComparison.A:\n",
    "      return 1\n",
    "    elif result is AnswerComparison.B:\n",
    "      return -1\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "  def __eq__(self, other):\n",
    "    \"\"\"Equality check that performs pairwise evaluation.\"\"\"\n",
    "    if not isinstance(other, QAGuidancePrompt):\n",
    "      return NotImplemented\n",
    "\n",
    "    return self._compare_all(other) == 0\n",
    "\n",
    "  def __lt__(self, other):\n",
    "    \"\"\"Ordering check that performs pairwise evaluation.\"\"\"\n",
    "    if not isinstance(other, QAGuidancePrompt):\n",
    "      return NotImplemented\n",
    "\n",
    "    return self._compare_all(other) < 0\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
